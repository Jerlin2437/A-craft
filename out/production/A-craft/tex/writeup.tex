\documentclass{article}
\usepackage[margin=1in]{geometry} % Set 1-inch margins on all sides
\usepackage{amsmath}

\title{Exploring A star and Star Craft}
\author{Jerlin Yuen (NetID: jy823, RUID: 214007343) and Mick Estiler (NetID: me6969, RUID: 589)}
\date{\today} % or specify a date

\begin{document}

\maketitle

\section{Introduction}
To preface, we have decided to use DFS as our main method to generate our maze. We have used the repository, https://github.com/algoprog/Laby, to base our own implementation of the maze generation in our project. We mainly used Chris's implementation since it also included a start and end node within the 2d 101x101 maze matrix.

\section{Question 1}
Here, we will explore a bit about the A star algorithm.

\paragraph{Part A}

We want to explore the reasoning of why A* moves east instead of north in Figure 8 of the Homework 1 writeup.

First, we want to recognize that the algorithm does not know which nodes are blocked and which are unblocked. Only neighbouring nodes are visible (immediate north, south, west, east nodes). In figure 8, there are 3 possible nodes that can be traversed at the start, each of which is unblocked. They are: \((E, 1)\), \((E, 3)\), \((D, 2)\) The heuristic \(h(n)\) that will be used will be Manhattan distances. \(g(n)\) will simply be the total traveled distance. 

At the start, \(g(n)\) is obviously 0. \(f(n) = h(n) + g(n)\), but \(g(n) = 0\), so \(f(n) = h(n)\). A* explores the path with the lowest \(h(n)\). At \((E,1)\), \(h(n) = 4\). At \((D,2)\), \(h(n) = 4\). At \((E,3)\), \(h(n) = 2\). Obviously, \(h(n) = f(n)\) at \((E,3)\) is the smallest value so the agent moves to the east first.

\paragraph{Part B}
We want to give a convincing argument that the agent in finite gridworlds indeed either reaches the target or discovers that this is impossible in finite time. And also prove that the number of moves of the agent until it reaches the target or discovers that this is impossible is bounded from above by the number of unblocked cells squared. 
\\
\\
First let's give a convincing argument that the agent in a finite girdworld will reach the target given that there is no blocked cell between the agent and the target. 

\textbf{Claim 1:} If there are no blocked cells separating the agent and the target in a finite gridworld, then the agent is guaranteed to reach the target.

\textbf{Proof:} Let $P$ be the set of all possible paths between the agent and the target. Since there are no blocked cells that barr agent from reaching target, $P$ is non-empty since there has to exist at least one path. As the gridworld is finite, $P$ is also finite. Therefore, the agent can explore each path in $P$ one by one, ensuring it reaches the target or determines impossibility in finite time.
\\
\\
Now, we prove that an agent in a finite world will discover that it will not reach the given target in finite time given that there is no path from the agent to the target.

\textbf{Claim 2:} The exploration process of the agent in a finite gridworld either reaches the target or discovers impossibility in finite time.

\textbf{Proof:} Since there is a finite number of paths to explore in the gridworld, the agent can explore each path one by one. A* algorithm in this exploration uses a consistent manhattan heuristic, which maintains a closed list ensuring that A* does not revisit any nodes. Since there is a finite amount of posible nodes to visit, we will explore all possible nodes, which will finish in finite time.
\\
\\
Now we prove that the number of max moves is less than the number of unblocked cells squared. 

\textbf{Claim 3:} The number of moves of the agent until it reaches the target or discovers impossibility is bounded from above by the number of unblocked cells squared.

\textbf{Proof:} Let $n$ be the number of unblocked cells. Because A* uses a consistent heuristic, the agent visits each unblocked cell at most once even in its worst case scenario. Therefore, the number of moves is at most $n$. Since there are $n$ unblocked cells, the total number of moves is always less than $n \times n = n^2$. 
\\ 
\\

\section{Question 2 - Exploring Tie-Breaking Methods in Priority Queues}

In this section, we delve into the concept of implementing different methods to resolve ties within a min priority queue. Our focus is on a queue that organizes nodes based on their $f$ value, ensuring that the node with the smallest $f$ value is retrieved when polling the queue. However, scenarios arise where two $f$ values are exactly equal, necessitating a tie-breaking mechanism to maintain queue order.

To address this, we consider prioritizing nodes based on their $g$ values as a tie-breaker. Given that our implementation is in Java, this tie-breaking logic is incorporated into the \texttt{compareTo} function. Specifically:

\begin{itemize}
    \item \textbf{Prioritizing Smaller $g$ Values:} If our goal is to prefer nodes with smaller $g$ values in the event of a tie, the \texttt{compareTo} function can be implemented as follows: \texttt{return Double.compare(this.g, o.g);}.
    \item \textbf{Prioritizing Larger $g$ Values:} Conversely, to prefer nodes with larger $g$ values when a tie occurs, we would adjust the \texttt{compareTo} function to: \texttt{return -Double.compare(this.g, o.g);}.
\end{itemize}

This approach allows us to fine-tune our priority queue's behavior, ensuring that it aligns with the specific requirements of our algorithm or use case.

\subsection{Runtime Analysis in a 101x101 Grid}
In our 101x101 grid that was run 50 times, the average runtime for each grid search is as follows:
\begin{itemize}
    \item For prioritizing smaller $g$ values, the time each grid took was 9.384355004 seconds, and explored 2,282,065 nodes .
    \item For prioritizing larger $g$ values, the time each grid took was 8.03633 seconds and explored 401,241.10 nodes.
\end{itemize}

The difference in runtime can be attributed to the grid's design. Larger, more complex grids, such as our 101x101 scenario with over 10,000 squares, seem to favor algorithms that resemble depth-first search (DFS). When prioritizing larger $g$ values, the algorithm explores nodes that are farther away, akin to how DFS operates by delving deeper into one path before backtracking.

\subsection{Implications of Prioritizing Larger $g$ Values}
This method appears to run more efficiently because the goal state is likely very far from the start state. Such a strategy shares similarities with the depth-first search approach, which often runs faster in large, complex search spaces. DFS explores as far as possible along each branch before backtracking, which can be more efficient in scenarios where the goal state is distant. This minimizes the number of nodes visited by not fully exploring every level before moving to the next.

Consequently, prioritizing larger $g$ values in our priority queue inadvertently adopts a strategy akin to DFS. This is particularly beneficial in our grid search problem, where the path to the goal is long and winding. It leverages the advantages of DFS, such as avoiding the overhead associated with maintaining a large frontier, which is common in breadth-first search (BFS). This tailored approach enhances our algorithm's efficiency in navigating through the complexities of the grid.

\subsection{Further Explorations of Different Methods to Break Ties}
In addition to direct comparisons between \(g\) values for tie-breaking, another approach involves assigning each node a single priority value. This method deviates from ordering nodes based purely on their smallest \(f\) value, relying instead on a pre-calculated priority that incorporates both \(f\) and \(g\) values. This strategy offers nuanced control over the prioritization process, particularly regarding how \(f\) and \(g\) values contribute to a node's overall priority.

\subsubsection{Preferring Smaller \(g\) Values with a Single Priority Value}
To implement an algorithm that prefers smaller \(g\) values using a single priority value, we adjust the formula to favor nodes with lower \(g\) values. The formula for preferring smaller \(g\) values is:
\begin{verbatim}
    final double c = 20000; // Constant larger than any possible g-value in the grid.
    
    // Calculate priority values incorporating both f and g, favoring smaller g-values.
    double thisPriority = c * this.f + this.g;
\end{verbatim}
This approach ensures that among nodes with the same \(f\) value, those with smaller \(g\) values receive higher priority.

\subsubsection{Preferring Larger \(g\) Values with a Single Priority Value}
Conversely, to prefer larger \(g\) values, we use a formula that inversely relates the \(g\) value's effect, rewarding higher \(g\) values with more prioritized positions. The formula for larger \(g\) values is:
\begin{verbatim}
    final double c = 20000; // Constant larger than any possible g-value in the grid.
    
    // Calculate priority values incorporating both f and g, favoring larger g-values.
    double thisPriority = c * this.f - this.g;
\end{verbatim}
This method prioritizes nodes with larger \(g\) values when \(f\) values are equal, aligning with strategies that benefit from deeper or costlier path explorations.

\subsubsection{Impact of the Constant \(C\) on Priority Calculation}
The constant \(C\) significantly influences the priority calculation for both smaller and larger \(g\) values. The choice of \(C\) allows for the fine-tuning of \(f\) and \(g\) values' relative impact:
\begin{itemize}
    \item \textbf{Higher \(C\) Value:} Increasing \(C\) minimizes the relative impact of \(g\) value differences, emphasizing the \(f\) value's dominance in the priority calculation. This setting is more neutral regarding \(g\) value preferences, focusing on \(f\) value disparities.
    \item \textbf{Lower \(C\) Value:} Lowering \(C\) increases the significance of \(g\) value differences in the priority calculation. For smaller \(g\) values, this makes the algorithm more sensitive to \(g\) value reductions. For larger \(g\) values, it accentuates the benefit of higher \(g\) values in tie-breaking scenarios.
\end{itemize}

Adjusting \(C\) provides a mechanism to balance the influence of \(f\) and \(g\) values in determining node priority, allowing the algorithm to be customized to specific search problem requirements or optimization goals. In our example, of 101x101 grids, the \(C\) should be greater than 10,201.

\subsubsection{Runtime Analysis and Algorithm Efficiency}
The efficiency of algorithms using this priority calculation method varies based on grid design and implementation specifics. The analysis of runtime data for both preferences reveals how different \(C\) values and the emphasis on \(g\) values affect algorithm performance across various configurations. In our 101x101 grid, conducted over 50 runs, the average runtime for each grid search is as follows:
\begin{itemize}
    \item For prioritizing smaller \(g\) values with a single priority value, the average time each grid took was 9.61443 seconds and explored 2,497,440.54 nodes.
    \item For prioritizing larger \(g\) values with a single priority value, the average time each grid took was 8.584355004 seconds, and explored 309,217.92 nodes.
\end{itemize}
Interestingly, the trend of prioritizing larger \(g\) values leading to a faster result, as compared to prioritizing smaller \(g\) values, persists here. However, the performance times when using an integer as a priority value are slightly longer compared to direct comparison. This could be attributed to the extra CPU cycles required to calculate the priority value or it may be a result of the relatively small sample size. 




\section{Question 3 - Comparing Forwards vs. Backwards A*}
We also used the tiebreak method to favor larger g values. If the g values are equal, we decided randomly on which one to explore. 

\begin{itemize}
    \item For Forwards A* the average time each grid took was 9.11443 seconds and explored 421,140.14 nodes.
    \item For Backwards A* the average time each grid took 10.072 seconds and explored 3,652,076.38 nodes. 
\end{itemize}

It seems there there is a significant difference between nodes explored in backwards vs forwards A star. There could be a few explanations as to why. 
\subsection{Forwards A*}
Forwards A* starts from the agents current position and aims to reach the goal. This may be a lot more intuitive as it simulates the perspective of an agent and is continually moving forward. It may also encounter obstacles A LOT sooner. Since when the maze is first starting, obstacles near the goal state are almost invisible whereas obstacles near the start state are likely to be seen. Remember that we can only see obstacles if the agent travels to there in run(), not through the hypothetical path computed in computePath(). 

\subsection{Backwards A*}
Backwards A* starts from the goal position and works back to the agent's current location. Can be less intuitive since it involves thinking in reverse but can be advantageous in certain scenarios, such as when the area near the goal is less obstructed and the initial path planning phase can proceed with fewer interruptions. Since the goal area is surrounded by obstacles we can't see yet, Repeated Backward A* might expand fewer nodes initially, as it can more freely establish a path before encountering denser obstacle distributions. 

\subsection{Comparison}
While Backwards and Forwards A* algorithms show negligible efficiency differences on small grids, their performance diverges on larger grids. Backwards A* tends to lead to suboptimal efficiency as it explores nodes near the goal first, potentially ignoring obstacles near the start. Consequently, on large grids, it may find the quickest unblocked path from the goal to an undefined start area, requiring navigation around the entire grid for a suitable entrance into the start node. In contrast, Forwards A* prioritizes exploring paths away from obstacles near the start, resulting in a potentially more direct route but encountering more obstacles near the goal. Thus, the choice between the two algorithms should consider the grid's size and complexity, as well as the trade-offs between initial exploration strategies and overall pathfinding efficiency.

\subsubsection{Goal Visibility}
In some scenarios, the visibility of the goal from the start position or vice versa can impact the effectiveness of each algorithm. For instance, if the goal is highly visible from the start position, Forwards A* might have an advantage as it can quickly identify a path towards the goal. Conversely, if the start position is highly visible from the goal, Backwards A* might have an advantage as it can quickly identify a path towards the start.

\section{Question 4 - Heuristics in the Adaptive A*}

\subsection{Consistency of Manhattan Distances in Gridworlds}
This subsection provides a proof that Manhattan distances are consistent in gridworlds where an agent can only move in the four cardinal directions: north, south, east, and west. The Manhattan distance is calculated by the following formula: \[d(A, B) = |x_1 - x_2| + |y_1 - y_2|\]
A heuristic is consistent if for every state \(n\) and every successor state \(succ(n)\) of \(n\) generated by an action \(a\), the estimated cost of reaching the goal from \(n\) is no greater than the cost of getting to \(succ(n)\) from \(n\) plus the estimated cost of reaching the goal from \(succ(n)\). Formally:
\[h(n) \leq c(n, a, succ(n)) + h(succ(n))\]
where \(h(n)\) is the heuristic estimate from \(n\) to the goal, \(c(n, a, succ(n))\) is the cost of moving from \(n\) to \(succ(n)\) using action \(a\), and \(h(succ(n))\) is the heuristic estimate from \(succ(n)\) to the goal.

In gridworlds where the agent can move in the four cardinal directions with a uniform cost of 1 for each move, the Manhattan distance directly corresponds to the minimum number of steps required to move from one cell to another. 

\subsection{Proof of Consistency}

To show consistency, consider a state \(s\) and a successor state \(succ(s)\) resulting from an action \(a\) in \(s\). Since each move has a cost of 1:
\[c(s, a, succ(s)) = 1\]

Moving from \(s\) to \(succ(s)\) in any cardinal direction reduces the Manhattan distance to the goal by exactly 1. Therefore:
\[h(s) - h(succ(s)) = 1\]

This implies:
\[h(s) = 1 + h(succ(s))\]

Satisfying the consistency condition:
\[h(n) \leq c(n, a, succ(n)) + h(succ(n))\]

Therefore, Manhattan distances are consistent in gridworlds where movement is restricted to the four main compass directions. This property is crucial for ensuring the forward, backward, and adaptive A* search algorithms using Manhattan distances as heuristics are both complete and optimal, minimizing the number of nodes expanded to find the shortest path to the goal.

\subsection{Admissibility and Consistency of Adaptive A*}
This subsection's goal is to provide proof that Adaptive A* leaves initially consistent \(h\)-values consistent even if action costs can increase. A heuristic \(h(s)\) is admissible if it never overestimates the cost to reach the goal from any node \(s\). So for every node \(s\): 
\[h(s) \leq h^*(s)\]

where \(h^*(s)\) is the true cost from \(s\) to the goal. The definition used for consistency will be the same as in section 5.1.

\subsection{Proof of Consistency}
Adaptive A* updates the heuristic values after each search based on the discovered costs from nodes to the goal. Specifically, after finding a path to the goal, the heuristic \(h_{\text{new}}(s)\) for each node \(s\) that was expanded during the search is updated to reflect the newly discovered costs. The updated heuristic is calculated as follows:
\[h_{\text{new}}(s) = g(s_{\text{goal}}) - g(s)\]

where \(g(s_{\text{goal}})\) is the cost from the start state to the goal state, determined by the path found in the most recent search, and \(g(s)\) is the cost from the start state to node \(s\), also determined in the most recent search. This calculation ensures that \(h_{\text{new}}(s)\) reflects the exact cost to reach the goal from \(s\) minus the cost already incurred to reach \(s\), making it an admissible heuristic because it does not overestimate the true cost to reach the goal.

To prove that \(h_{\text{new}}(s)\) is consistent, we need to show that for any node \(s\) and its successor \(succ(s)\) with action cost \(c(s, a, succ(s))\), the following condition holds:
\[h_{\text{new}}(s) \leq c(s, a, succ(s)) + h_{\text{new}}(succ(s))\]

Given the definition of \(h_{\text{new}}(s)\), we substitute the expressions for \(h_{\text{new}}(s)\) and \(h_{\text{new}}(succ(s))\):
\[g(s_{\text{goal}}) - g(s) \leq c(s, a, succ(s)) + \left(g(s_{\text{goal}}) - g(succ(s))\right)\]

Since \(g(succ(s)) = g(s) + c(s, a, succ(s))\), we substitute \(g(succ(s))\) into the inequality:
\[g(s_{\text{goal}}) - g(s) \leq c(s, a, succ(s)) + g(s_{\text{goal}}) - \left(g(s) + c(s, a, succ(s))\right)\]

Simplifying both sides of the inequality, we find that both sides are equivalent, proving the consistency of \(h_{\text{new}}(s)\).

Therefore, the update mechanism of Adaptive A* ensures that the heuristic values remain both admissible and consistent, even as action costs increase. This guarantees that Adaptive A* will continue to find the optimal path to the goal while potentially reducing the number of nodes expanded in subsequent searches, as the heuristic values become more informed.



\section{Question 5 - Analysis of Repeated Forward A* and Adaptive A* Performance}
In our experimental comparison between Repeated Forward A* and Adaptive A*, both algorithms were configured to break ties among cells with the same \(f\)-value in favor of cells with larger \(g\)-values. Any remaining ties broken randomly. The algorithms were evaluated based on the average number of nodes explored and the average runtime across 50 runs. Here are the observed results:

\subsection{Observed Results}

\begin{itemize}
    \item \textbf{Repeated Forward A*}:
    \begin{itemize}
        \item Average nodes explored: 321,618.78
        \item Average time: 9.9985 seconds
    \end{itemize}
    \item \textbf{Adaptive A*}:
    \begin{itemize}
        \item Average nodes explored: 310,153.78
        \item Average time: 10.0492 seconds
    \end{itemize}
\end{itemize}

\subsection{Analysis of Observations}

The results indicate that, on average, Adaptive A* explores fewer nodes than Repeated Forward A* across 50 runs. This could suggest that Adaptive A* is slightly more efficient in navigating the search space, potentially due to its mechanism of updating heuristic values based on previous searches, which may allow it to make more informed decisions in later runs.

However, despite exploring fewer nodes, Adaptive A* exhibits a marginally higher average runtime than Repeated Forward A*. This counterintuitive observation can be attributed to several factors. Adaptive A* updates the heuristic values of nodes based on the results of each search. This process, while beneficial for reducing the number of explored nodes, introduces computational overhead that is not present in Repeated Forward A*. Also, although both algorithms use the same tie-breaking strategy, the dynamics of how ties are encountered and resolved can differ due to the adaptive nature of the heuristic in Adaptive A*. This could influence the runtime in subtle ways.

\subsection{Conclusion}

The observation that Adaptive A* explores fewer nodes but takes slightly longer to run than Repeated Forward A* underscores the complexity of algorithm performance analysis. It highlights that node exploration efficiency does not always directly correlate with runtime efficiency. The computational overhead being introduced after every run in Adaptive A* is definitely the main factor in slowing down the overall performance of the search algorithm.


\section{Question 6 - Statistical Hypothesis Testing for Runtime Differences}

Following the experimental comparison in Question 5 between Repeated Forward A* and Adaptive A* regarding their runtimes, we observed that Adaptive A* had a slightly higher average runtime over 50 runs, despite exploring fewer nodes on average. To determine whether this observed difference in runtime is statistically significant or merely a result of sampling variability, we propose conducting a statistical hypothesis test.

\subsection{Formulating Hypotheses}

\begin{itemize}
    \item \textbf{Null Hypothesis} (\(H_0\)): There is no significant difference in the runtime between Repeated Forward A* and Adaptive A*. Any observed differences are attributable to sampling noise.
    \item \textbf{Alternative Hypothesis} (\(H_A\)): There is a significant difference in the runtime between Repeated Forward A* and Adaptive A*, indicating a systematic difference in performance.
\end{itemize}

\subsection{Set \(\alpha\)}
We set the level of significance, \(\alpha\), to 0.05. This value represents the threshold for rejecting the null hypothesis, indicating a 5\% risk of concluding a difference when there is none.

\subsection{Collect Data}
Data consists of the average runtimes of both algorithms over 50 runs:
\begin{itemize}
    \item Repeated Forward A*: Average runtime over 50 runs.
    \item Adaptive A*: Average runtime over 50 runs.
\end{itemize}

\subsection{Calculate a Test Statistic}
Perform an independent two-sample t-test to calculate the test statistic. This involves comparing the mean runtimes of the two algorithms to assess if the observed difference is statistically significant given the variance and sample size.

\subsection{Construct Acceptance / Rejection Regions}
The rejection region is defined by the critical value associated with the chosen \(\alpha\). If the calculated test statistic falls into the rejection region (typically, if the p-value \(\leq \alpha\)), we reject \(H_0\).

\subsection{Conclusion about \(H_0\)}
\begin{itemize}
    \item If the p-value \(\leq 0.05\), reject \(H_0\) in favor of \(H_A\), concluding there is a statistically significant difference in runtime.
    \item If the p-value \(> 0.05\), fail to reject \(H_0\), indicating that any observed differences could be attributed to chance.
\end{itemize}

This structured approach to hypothesis testing allows us to quantitatively assess whether the differences in runtime between Repeated Forward A* and Adaptive A* are statistically significant or if they could merely be the result of sampling variability.


\end{document}
